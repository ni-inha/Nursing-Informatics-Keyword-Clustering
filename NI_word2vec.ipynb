{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb2001c-5946-40e8-8d2c-97ed49889627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from inflection import singularize\n",
    "#import inflect\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pandas.core.arrays import StringArray\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dddaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_2018 = pd.read_csv('data/processed_data/ngram_data_frequency_2018.csv') #,encoding='cp949'\n",
    "index = csv_2018['index_1'].dropna()\n",
    "list = []\n",
    "for i in range(len(index)):\n",
    "    if(csv_2018['Flag_1'][i]== 0 and csv_2018['Flag_11'][i]=='X'):\n",
    "        list.append(csv_2018['bigram/trigram'][i])\n",
    "series = pd.Series(list)\n",
    "bi_tri_dict_2018 = series.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63eb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = csv_2018['index_2'].dropna()\n",
    "list = []\n",
    "for i in range(len(index)):\n",
    "    if(csv_2018['Flag_2'][i]== 0 and csv_2018['Flag_22'][i]=='X'):\n",
    "        list.append(csv_2018['unigram'][i])\n",
    "series = pd.Series(list)\n",
    "uni_dict_2018 = series.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feda8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_2021 = pd.read_csv('data/processed_data/ngram_data_frequency_2021.csv') #,encoding='cp949'\n",
    "index = csv_2021['index_1'].dropna()\n",
    "list = []\n",
    "for i in range(len(index)):\n",
    "    if(csv_2021['Flag_1'][i]=='X'):\n",
    "        list.append(csv_2021['bigram/trigram'][i])\n",
    "series = pd.Series(list)\n",
    "bi_tri_dict_2021 = series.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2db2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = csv_2021['index_2'].dropna()\n",
    "list = []\n",
    "for i in range(len(index)):\n",
    "    if(csv_2021['Flag_2'][i]=='X'):\n",
    "        list.append(csv_2021['unigram'][i])\n",
    "series = pd.Series(list)\n",
    "uni_dict_2021 = series.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a6bf69-4a5b-4d50-bdce-bdbdd9bbda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('data/raw_data/datalist_2016.csv',encoding='cp949') #,encoding='cp949'\n",
    "abstract = csv['초록']\n",
    "abstract_values = []\n",
    "abstract_val = abstract.values\n",
    "for i in range(len(abstract_val)):  #nan 값 제외하기\n",
    "  if (abstract_val[i] == abstract_val[i]):\n",
    "    abstract_values.append(abstract_val[i])\n",
    "\n",
    "final_values = []\n",
    "for i in range(len(abstract_values)):\n",
    " final_values.append(re.sub(r'[0-9]',\"\",abstract_values[i])) #https://engineer-mole.tistory.com/238 (문자열에서 특정 문자만 삭제하는 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7abbb6e-c2bc-4161-b2e6-e20767bc82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_val = np.array(final_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f57b3e4-7266-4bbb-af5e-884e3c941545",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eae701e-447f-49b7-b318-ac93ff5ed7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(dict):\n",
    "    list = []\n",
    "    for key in dict.keys():\n",
    "        list.append(key)\n",
    "         \n",
    "    return list\n",
    "#https://www.geeksforgeeks.org/python-get-dictionary-keys-as-a-list/ 딕셔너리를 리스트로 받기\n",
    "def replace(list):    \n",
    "    result_list = []\n",
    "    for i in range(len(list)):\n",
    "      new_str = [s.replace(' ','_') for s in list[i]] #https://github.com/RaRe-Technologies/gensim/issues/388 문자열에 whitespace가 있으면 에러가 생김\n",
    "      result_list.append(new_str)\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "def replace_nf(list):\n",
    "    new_str = [s.replace(' ','_') for s in list] #https://github.com/RaRe-Technologies/gensim/issues/388 문자열에 whitespace가 있으면 에러가 생김\n",
    "\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc5b2d60-fcd6-426e-809e-bb3e2442550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec = CountVectorizer(stop_words=stop_word, ngram_range=(1,3)) #https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5 Text analysis with ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa4d7e3b-78f8-44cb-a6af-2d3517fbd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(final_val)):\n",
    "  sent = []\n",
    "  c_vec.fit_transform([final_val[i].astype('U')]) #여기에서 c_vec에 ngram화된 단어들이 입력됨, U로 유니코드 문자열로 전환해줌\n",
    "  sent.append(c_vec.vocabulary_)\n",
    "  ngrams.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6018f02-be5f-41b3-94a9-e4caaf2c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(ngrams)):\n",
    "  results.append(getList(ngrams[i][0])) #뒤에 [0]을 붙이면 딕셔너리의 키값이 읽힘 , 리스트에서 문자열로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b4ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym = pd.read_csv('data/processed_data/synonym.csv') #,encoding='cp949'\n",
    "original = synonym['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13441960",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(len(results)):\n",
    "    words = results[i]\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        if(word in original):\n",
    "            word = synonym.loc[synonym['word']==word,'replace']\n",
    "            word_list.append(word)\n",
    "        else:\n",
    "            word_list.append(word)\n",
    "    output.append(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2b34d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_tri_dict = bi_tri_dict_2018.to_list() + bi_tri_dict_2021.to_list()\n",
    "uni_dict = uni_dict_2018.to_list() + uni_dict_2021.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "122bd461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 273/273 [00:06<00:00, 41.27it/s]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in tqdm(range(len(output))):\n",
    "    words = output[i]\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        if(word not in bi_tri_dict and word not in uni_dict):\n",
    "            word_list.append(word)\n",
    "    result.append(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1da58c9a-ce31-4622-8fe8-0304b30dcdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_low = []\n",
    "for i in range(len(result)):\n",
    "    lowcase = []\n",
    "    for word in result[i]:\n",
    "     lowcase.append(singularize(word))\n",
    "    result_low.append(lowcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1bf74-0c99-4656-986e-7dd9ef2e085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = replace(result_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae166ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17b375-8dbb-4ef4-abe2-82bd542dfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918df07-61c4-41ab-8016-fe37738399cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(preprocessed_sentences)\n",
    "model = Word2Vec(bigram_transformer[preprocessed_sentences],window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd47c37-4e9d-4466-b80d-4f6c9269cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_phr2021_0404')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fae6cb-d6cf-4317-8474-079158a220be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input eng_phr2021_0404 --output eng_phr2021_0404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21751713-e429-41b3-8a1d-99cbfb479046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_kernel",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
